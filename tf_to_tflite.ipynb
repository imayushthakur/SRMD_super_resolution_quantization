{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def pad_to_fit(image, pad_height, pad_width):\n",
    "    inp_h, inp_w,c = image.shape\n",
    "    num_horizontal = math.ceil(inp_h/pad_height)\n",
    "    num_vertical = math.ceil(inp_w/pad_width)\n",
    "    \n",
    "    out_h = num_horizontal*pad_height\n",
    "    out_w = num_vertical*pad_width\n",
    "    \n",
    "    out_img = np.zeros((out_h, out_w, c), dtype=image.dtype)    \n",
    "    out_img[0:inp_h, 0:inp_w, :] = image\n",
    "    \n",
    "    return out_img, num_horizontal, num_vertical\n",
    "\n",
    "def get_tiles(image, tile_height, tile_width):\n",
    "    \n",
    "    image_height, image_width, _ = image.shape\n",
    "    serial_no = 0\n",
    "    for column_idx, i in enumerate(range(0, image_height, tile_height), 1):\n",
    "        for row_idx, j in enumerate(range(0, image_width, tile_width), 1):\n",
    "            serial_no += 1\n",
    "            top = i\n",
    "            left = j\n",
    "            bottom = i+tile_height\n",
    "            right = j+tile_width\n",
    "            yield (column_idx, row_idx), (top, left, bottom, right), image[i:i+tile_height, j:j+tile_width, :]\n",
    "    \n",
    "\n",
    "def common(img):\n",
    "    h_input, w_input = img.shape[0:2]\n",
    "    img = img.astype(np.float32)\n",
    "    if np.max(img) > 256:  # 16-bit image\n",
    "        max_range = 65535\n",
    "        print('\\tInput is a 16-bit image')\n",
    "    else:\n",
    "        max_range = 255\n",
    "    img = img / max_range\n",
    "    if len(img.shape) == 2:  # gray image\n",
    "        img_mode = 'L'\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    elif img.shape[2] == 4:  # RGBA image with alpha channel\n",
    "        img_mode = 'RGBA'\n",
    "        alpha = img[:, :, 3]\n",
    "        img = img[:, :, 0:3]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    else:\n",
    "        img_mode = 'RGB'\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    return img, h_input, w_input\n",
    "\n",
    "\n",
    "def preprocess_image(img, pre_pad=10, mod_scale=4):\n",
    "    img, h_input, w_input = common(img)\n",
    "    (left, right, top, bottom) = (0, pre_pad, 0, pre_pad)\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REFLECT, None, value=0)\n",
    "   \n",
    "    mod_pad_h, mod_pad_w = 0, 0\n",
    "    h,w,_ = img.shape\n",
    "    if (h % mod_scale != 0):\n",
    "        mod_pad_h = (mod_scale - h % mod_scale)\n",
    "    if (w % mod_scale != 0):\n",
    "        mod_pad_w = (mod_scale - w % mod_scale)\n",
    "\n",
    "    (left, right, top, bottom) = (0, mod_pad_w, 0, mod_pad_h)\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_REFLECT, None, value=0)\n",
    "    img = np.transpose(img, (2,0,1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    return img, mod_pad_h, mod_pad_w\n",
    "\n",
    "\n",
    "def post_process(output, mod_pad_h, mod_pad_w, mod_scale=4, pre_pad=10):\n",
    "    scale=1 # hardcoded for model_scale=4\n",
    "    # remove extra pad\n",
    "    output = np.squeeze(output)\n",
    "    output = np.transpose(output, (1,2,0))\n",
    "    if mod_scale is not None:\n",
    "        h, w, _ = output.shape\n",
    "        output = output[0:h - mod_pad_h * scale, 0:w - mod_pad_w * scale, :]\n",
    "    # remove prepad\n",
    "    if pre_pad != 0:\n",
    "        # _, _, h, w = output.size()\n",
    "        h, w, _ = output.shape\n",
    "        output = output[0:h - pre_pad * scale, 0:w - pre_pad * scale, :]\n",
    "    # unsqueeze to remove batch\n",
    "    output = np.clip(output, 0, 1)\n",
    "    # convert to channel last\n",
    "    output = (output * 255.0).round().astype(np.uint8)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def run_inference(image_path, model, input_height, input_width, NETSCALE = 4):\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # calculate original height and wisth to crop the final result image to required output dimension\n",
    "    orig_height, orig_width, _ = image.shape\n",
    "    image, num_horizontal, num_vertical = pad_to_fit(image, pad_height=input_height, pad_width=input_width)\n",
    "    total_tiles = num_horizontal*num_vertical\n",
    "    height, width, _ = image.shape\n",
    "    print(f\"padding done ({orig_height} x {orig_width}) -> ({height} x {width})\")\n",
    "        \n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    num_processed = 0\n",
    "    print(\"\")\n",
    "    \n",
    "    for (column_idx, row_idx), (top, left, bottom, right), image_tile in get_tiles(image, tile_height=input_height, tile_width=input_width):\n",
    "        in_h, in_w, _ = image_tile.shape\n",
    "   \n",
    "        input_data, mod_pad_h, mod_pad_w = preprocess_image(image_tile)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        res_array = post_process(output_data, mod_pad_h, mod_pad_w)\n",
    "        res_array = res_array[:in_h*NETSCALE, :in_w*NETSCALE, :]\n",
    "        out_h, out_w, _ = res_array.shape\n",
    "        \n",
    "        h_ratio = out_h/in_h\n",
    "        w_ratio = out_w/in_w\n",
    "        \n",
    "    \n",
    "        results.append({\"array\":res_array, \"coords\":(int(top*h_ratio), int(left*h_ratio), int(bottom*h_ratio), int(right*w_ratio)), \"position\":(column_idx, row_idx), })\n",
    "        num_processed += 1\n",
    "        print(f\"\\rprocessed {num_processed}/{total_tiles} tiles\")#, end=\"\\r\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"inference took {} seconds\".format(end-start))\n",
    "    \n",
    "    return results, num_horizontal, num_vertical, orig_height, orig_width\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_model/srmd_android.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "output_shape = output_details[0]['shape']\n",
    "\n",
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a94d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_PATH = f\"images/people_{i+1}.jpg\"\n",
    "result, num_horizontal, num_vertical, orig_height, orig_width = run_inference(IMAGE_PATH, interpreter, input_height=179, input_width=179)\n",
    "NETSCALE = 4\n",
    "sample = result[0][\"array\"]\n",
    "sample_height, sample_width, _ = sample.shape\n",
    "result_array = np.zeros((sample_height*num_horizontal, sample_width*num_vertical, 3), dtype=np.uint8)\n",
    "index = 0\n",
    "\n",
    "for r in result:\n",
    "    array = r[\"array\"]\n",
    "    height, width, _ = array.shape\n",
    "    top, left, bottom, right = r[\"coords\"]\n",
    "    pos = r[\"position\"]\n",
    "    \n",
    "    result_array[top:bottom, left:right, :] = array\n",
    "        \n",
    "result_array = result_array[0:orig_height*NETSCALE, 0:orig_width*NETSCALE, :]\n",
    "result_array = cv2.cvtColor(result_array, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite(f\"real_esrgan_output/people_{i+1}.jpg\", result_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
